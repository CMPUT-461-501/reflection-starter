{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "# ! pip install -U spacy\n",
    "# ! python -m spacy download en\n",
    "# ! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import heapq\n",
    "import string\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "sent_detector = PunktSentenceTokenizer()\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")  # make sure to use larger package!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load in essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = set()\n",
    "human_V = set()\n",
    "human_word_count = 0\n",
    "human_sentences = []\n",
    "human_vectors = []\n",
    "\n",
    "llm_V = set()\n",
    "llm_word_count = 0\n",
    "llm_sentences =[]\n",
    "llm_vectors =[]\n",
    "\n",
    "with open('./human.txt') as f:\n",
    "    lines = []\n",
    "    for line in f:\n",
    "        lines.append(line)\n",
    "    text = ' \\n'.join(lines)\n",
    "    \n",
    "    for sentence in sent_detector.tokenize(text.strip()):\n",
    "        clean = sentence.lower().translate(translator)\n",
    "        human_sentences.append(clean)\n",
    "        words = word_tokenize(clean)\n",
    "        human_word_count += len(words)\n",
    "        for word in words:\n",
    "            human_V.add(word)\n",
    "    f.close()\n",
    "\n",
    "with open('./llm.txt') as f:\n",
    "    lines = []\n",
    "    for line in f:\n",
    "        lines.append(line)\n",
    "    text = ' \\n'.join(lines)\n",
    "    \n",
    "    for sentence in sent_detector.tokenize(text.strip()):\n",
    "        clean = sentence.lower().translate(translator)\n",
    "        llm_sentences.append(clean)\n",
    "        words = word_tokenize(clean)\n",
    "        llm_word_count += len(words)\n",
    "        for word in words:\n",
    "            llm_V.add(word)\n",
    "    f.close()\n",
    "\n",
    "V = human_V.union(llm_V)\n",
    "\n",
    "# Full text of essays\n",
    "ft_human = ' '.join(human_sentences)\n",
    "ft_llm = ' '.join(llm_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word usage stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Human wrote {t_h} words in total, with {u_h} unique.\\n\\nLLM wrote {t_l} words in total, with {u_l} unique.\"\n",
    "print(s.format(t_h=human_word_count, u_h=len(human_V), t_l=llm_word_count, u_l=len(llm_V)))\n",
    "\n",
    "counter = Counter(word_tokenize(ft_human))\n",
    "h = []\n",
    "for w in counter:\n",
    "    heapq.heappush(h,(counter[w],w))\n",
    "\n",
    "k=50\n",
    "print(\"\\n{k} least frequent words used in human essay:\".format(k=k))\n",
    "LEN = min(len(h),k)\n",
    "for i in range(LEN):\n",
    "    ans = heapq.heappop(h)\n",
    "    print(\"\\t{w}: {c} time(s)\".format(w=ans[1],c=ans[0]))\n",
    "\n",
    "counter = Counter(word_tokenize(ft_llm))\n",
    "h = []\n",
    "for w in counter:\n",
    "    heapq.heappush(h,(counter[w],w))\n",
    "\n",
    "print(\"\\n{k} least frequent words used in LLM essay:\".format(k=k))\n",
    "LEN = min(len(h),k)\n",
    "for i in range(LEN):\n",
    "    ans = heapq.heappop(h)\n",
    "    print(\"\\t{w}: {c} time(s)\".format(w=ans[1],c=ans[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions to compute cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentence, V):\n",
    "    v = []\n",
    "    counter = Counter(word_tokenize(sentence))\n",
    "    for w in V:\n",
    "        if w in counter:\n",
    "            v.append(1+math.log(counter[w]))\n",
    "        else:\n",
    "            v.append(0)\n",
    "    return v\n",
    "\n",
    "def norm(v):\n",
    "    return sum(a*a for a in v)**0.5\n",
    "\n",
    "def cosine(v1, v2):\n",
    "    # Calculate dot product\n",
    "    dot_product = sum(a*b for a, b in zip(v1, v2))\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = dot_product / (norm(v1) * norm(v2))\n",
    "    return cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cosine between whole essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=cosine(vectorize(ft_human,V),vectorize(ft_llm,V))\n",
    "print(\"cosine between essays: {c:.4f}\".format(c=c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# up to 5 most similar sentences between essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = []\n",
    "for i in range(len(human_sentences)):\n",
    "    for j in range(len(llm_sentences)):\n",
    "        sim = cosine(vectorize(human_sentences[i],V),vectorize(llm_sentences[j],V))\n",
    "        heapq.heappush(h,(1-sim,[i,j]))\n",
    "\n",
    "LEN = min(len(h),5)\n",
    "for i in range(LEN):\n",
    "    ans = heapq.heappop(h)\n",
    "    s = \"cosine: {s:.4f}\\nHuman: {h}\\nLLM: {l}\\n\"\n",
    "    print(s.format(s=1-ans[0],h=human_sentences[ans[1][0]],l=llm_sentences[ans[1][1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(ft_human)\n",
    "doc2 = nlp(ft_llm)\n",
    "print(\"SpaCy similarity between essays: {c:.4f}\".format(c=c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = []\n",
    "for i in range(len(human_sentences)):\n",
    "    for j in range(len(llm_sentences)):\n",
    "        doc1 = nlp(human_sentences[i])\n",
    "        doc2 = nlp(llm_sentences[j])\n",
    "        sim = doc1.similarity(doc2)\n",
    "        heapq.heappush(h,(1-sim,[i,j]))\n",
    "\n",
    "LEN = min(len(h),5)\n",
    "for i in range(LEN):\n",
    "    ans = heapq.heappop(h)\n",
    "    s = \"SpaCy sim: {s:.4f}\\nHuman: {h}\\nLLM: {l}\\n\"\n",
    "    print(s.format(s=1-ans[0],h=human_sentences[ans[1][0]],l=llm_sentences[ans[1][1]]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
